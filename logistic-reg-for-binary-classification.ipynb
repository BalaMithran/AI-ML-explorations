{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:40.031774Z","iopub.execute_input":"2022-01-08T15:34:40.032080Z","iopub.status.idle":"2022-01-08T15:34:40.040844Z","shell.execute_reply.started":"2022-01-08T15:34:40.032046Z","shell.execute_reply":"2022-01-08T15:34:40.040189Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv\")\n\nprint(\"Number of Datapoints :\",data.shape[0])\nprint(\"Number of Features/Columns :\",data.shape[1])\nprint(\"Features: \",data.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:40.042750Z","iopub.execute_input":"2022-01-08T15:34:40.043280Z","iopub.status.idle":"2022-01-08T15:34:40.119739Z","shell.execute_reply.started":"2022-01-08T15:34:40.043242Z","shell.execute_reply":"2022-01-08T15:34:40.118937Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:40.120691Z","iopub.execute_input":"2022-01-08T15:34:40.121049Z","iopub.status.idle":"2022-01-08T15:34:40.133320Z","shell.execute_reply.started":"2022-01-08T15:34:40.121019Z","shell.execute_reply":"2022-01-08T15:34:40.132267Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"data prep\n1. remove stopwords\n2. remove punctuation\n3. lower case","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\n\n#loading_the_stop_words_from_nltk_library_\nstop_words = set(stopwords.words('english'))\n\ndef txt_preprocessing(total_text, index, column, df):\n    if type(total_text) is not int:\n        string = \"\"\n        \n        #replace_every_special_char_with_space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        \n        #replace_multiple_spaces_with_single_space\n        total_text = re.sub('\\s+',' ', total_text)\n        \n        #converting_all_the_chars_into_lower_case\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        #if_the_word_is_a_not_a_stop_word_then_retain_that_word_from_the_data\n            if not word in stop_words:\n                string += word + \" \"\n        \n        df[column][index] = string\n        \n        \n#data_text_processing_stage_\nfor index, row in data.iterrows():\n    if type(row['text']) is str:\n        txt_preprocessing(row['text'], index, 'text', data)\n    else:\n        print(\"THERE IS NO TEXT DESCRIPTION FOR ID :\",index)\n\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:40.135240Z","iopub.execute_input":"2022-01-08T15:34:40.135738Z","iopub.status.idle":"2022-01-08T15:34:43.380700Z","shell.execute_reply.started":"2022-01-08T15:34:40.135693Z","shell.execute_reply":"2022-01-08T15:34:43.380024Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Select columns","metadata":{}},{"cell_type":"code","source":"preprocessed_data = pd.DataFrame({'text':data['text'], 'Spam/Ham':data['label_num']})\npreprocessed_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:43.382182Z","iopub.execute_input":"2022-01-08T15:34:43.382714Z","iopub.status.idle":"2022-01-08T15:34:43.393799Z","shell.execute_reply.started":"2022-01-08T15:34:43.382676Z","shell.execute_reply":"2022-01-08T15:34:43.392903Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Split data","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = preprocessed_data['text']\nY = preprocessed_data['Spam/Ham']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=0)\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=0)\n\nprint(\"NUMBER OF DATA POINTS IN TRAIN DATA :\", X_train.shape[0])\nprint(\"NUMBER OF DATA POINTS IN TEST DATA :\", X_test.shape[0])\nprint(\"NUMBER OF DATA POINTS IN CROSS VALIDATION DATA :\", X_cv.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:43.395190Z","iopub.execute_input":"2022-01-08T15:34:43.395512Z","iopub.status.idle":"2022-01-08T15:34:43.414624Z","shell.execute_reply.started":"2022-01-08T15:34:43.395465Z","shell.execute_reply":"2022-01-08T15:34:43.413681Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Text to vector","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntext_vec = TfidfVectorizer(min_df=10, max_features=5000)\ntext_vec.fit(X_train.values)\n# note that we are applying tfidf after train test split \n# because tfidf takes into account number of sentences in the dataset\ntrain_text = text_vec.transform(X_train.values)\ntest_text = text_vec.transform(X_test.values)\ncv_text = text_vec.transform(X_cv.values)\n\nprint(train_text.shape)\nprint(test_text.shape)\nprint(cv_text.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:43.416634Z","iopub.execute_input":"2022-01-08T15:34:43.416957Z","iopub.status.idle":"2022-01-08T15:34:44.317350Z","shell.execute_reply.started":"2022-01-08T15:34:43.416921Z","shell.execute_reply":"2022-01-08T15:34:44.315735Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"model training\nThis is a binary classification problem\ncan be solved in the foloowing ways\n1. Logistic Regression\n2. KNN\n3. Decision Tree\n4. SVM\n5. Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\n#train a logistic regression + calibration model using text features which are one-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42) #loss='log'_means_logistic_regression\n    clf.fit(train_text, y_train)\n    \n    lr_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    lr_sig_clf.fit(train_text, y_train)\n    \n    predict_y = lr_sig_clf.predict_proba(cv_text)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    \n    print('For values of alpha =',i,\"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n    \nplt.grid()\nplt.title(\"Cross Validation Error for Each Alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error Measure\")\nplt.show()\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text, y_train)\n\nlr_sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nlr_sig_clf.fit(train_text, y_train)\n\npredict_y = lr_sig_clf.predict_proba(train_text)\nprint('For Values of Best Alpha =', alpha[best_alpha],\"The Train Log Loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = lr_sig_clf.predict_proba(test_text)\nprint('For Values of Best Alpha =', alpha[best_alpha],\"The Test Log Loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\npredict_y = lr_sig_clf.predict_proba(cv_text)\nprint('For Values of Best Alpha =', alpha[best_alpha],\"The Cross Validation Log Loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:34:44.319588Z","iopub.execute_input":"2022-01-08T15:34:44.319834Z","iopub.status.idle":"2022-01-08T15:34:45.131891Z","shell.execute_reply.started":"2022-01-08T15:34:44.319804Z","shell.execute_reply":"2022-01-08T15:34:45.131125Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Measure Accuracy**\n","metadata":{}},{"cell_type":"markdown","source":"1. confusion matrix","metadata":{}},{"cell_type":"code","source":"#this_function_plots_the_confusion_matrices_given_y_i_and_y_i_hat_\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef plot_confusion_matrix(test_y, predict_y):\n\n    C = confusion_matrix(test_y, predict_y) #confusion_mat\n    A =(((C.T)/(C.sum(axis=1))).T) #recall_mat\n    B =(C/C.sum(axis=0)) #precision_mat\n    \n    labels = [0,1]\n    \n    #representing_C_in_heatmap_format\n    print(\"-\"*40, \"Confusion Matrix\", \"-\"*40)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    #representing_B_in_heatmap_format\n    print(\"-\"*40, \"Precision Matrix (Columm Sum=1)\", \"-\"*40)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    #representing_A_in_heatmap_format\n    print(\"-\"*40, \"Recall Matrix (Row Sum=1)\", \"-\"*40)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:35:31.325430Z","iopub.execute_input":"2022-01-08T15:35:31.325758Z","iopub.status.idle":"2022-01-08T15:35:31.485225Z","shell.execute_reply.started":"2022-01-08T15:35:31.325725Z","shell.execute_reply":"2022-01-08T15:35:31.483870Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_cv, lr_sig_clf.predict(cv_text.toarray()))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:35:36.990853Z","iopub.execute_input":"2022-01-08T15:35:36.991379Z","iopub.status.idle":"2022-01-08T15:35:37.924831Z","shell.execute_reply.started":"2022-01-08T15:35:36.991345Z","shell.execute_reply":"2022-01-08T15:35:37.923807Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"2. f score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_cv, lr_sig_clf.predict(cv_text)))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:35:43.924723Z","iopub.execute_input":"2022-01-08T15:35:43.925010Z","iopub.status.idle":"2022-01-08T15:35:43.939043Z","shell.execute_reply.started":"2022-01-08T15:35:43.924981Z","shell.execute_reply":"2022-01-08T15:35:43.938113Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":" 3. test accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nlr_test_accuracy = (lr_sig_clf.score(test_text, y_test)*100)\nprint(\"Logistic Regression Test Accuracy -\",lr_test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T15:35:48.761289Z","iopub.execute_input":"2022-01-08T15:35:48.761868Z","iopub.status.idle":"2022-01-08T15:35:48.773060Z","shell.execute_reply.started":"2022-01-08T15:35:48.761829Z","shell.execute_reply":"2022-01-08T15:35:48.771965Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}